{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTVh5gH91kL1"
   },
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install plotly\n",
    "!pip install pydantic\n",
    "!pip install pyyaml\n",
    "!pip install nltk\n",
    "!pip install gpt-2-simple\n",
    "!pip install wordcloud\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJAppzUD2A8t"
   },
   "outputs": [],
   "source": [
    "!sudo apt install -y --no-install-recommends g++ protobuf-compiler libprotobuf-dev\n",
    "!pip install gcld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3r57ZIm2aNl"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "71xVdhWe40br"
   },
   "outputs": [],
   "source": [
    "import gcld3\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gpt_2_simple as gpt2\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTDrQxo0__U_"
   },
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScGg1cDbABP4"
   },
   "outputs": [],
   "source": [
    "gpt2.download_gpt2(model_name=\"124M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "S9SVbuaf420Q"
   },
   "outputs": [],
   "source": [
    "COLUMNS = ['tweet', 'likes', 'retweet_count', 'user_screen_name', 'user_description', 'user_followers_count']\n",
    "LANG = 'en'\n",
    "TW_USERNAME_REGEX = r\"@[a-zA-Z0-9_]{0,15}\"\n",
    "URL_REGEX = r\"\\b(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][\" \\\n",
    "            r\"a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,\" \\\n",
    "            r\"}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\\b\"\n",
    "SPACES_REGEX = r\"\\s+\"\n",
    "HASHTAG_REGEX = r'^#[^ !@#$%^&*(),.?\":{}|<>]*$'"
   ],
   "metadata": {
    "id": "S9SVbuaf420Q"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4xHHZiW92HD"
   },
   "outputs": [],
   "source": [
    "GOOGLE_DRIVE_BASE_DIR = \"/content/drive/MyDrive/ITBA/Quinto Año/Segundo Cuatrimestre/NLP/TP\"\n",
    "gpt2.mount_gdrive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_8qbPI-45eN",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def get_selected_columns(df, columns):\n",
    "    return df[columns]\n",
    "\n",
    "\n",
    "def delete_hashtag_symbol(df):\n",
    "    df['tweet'] = df['tweet'].replace('#', '', regex=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def delete_twitter_username(df):\n",
    "    df['tweet'] = df['tweet'].replace(TW_USERNAME_REGEX, '', regex=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def delete_urls(df):\n",
    "    df['tweet'] = df['tweet'].replace(URL_REGEX, '', regex=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def delete_multiple_spaces(df):\n",
    "    df['tweet'] = df['tweet'].replace(SPACES_REGEX, '', regex=True)\n",
    "\n",
    "\n",
    "def is_lang(row, detector, lang='en'):\n",
    "    prediction = detector.FindLanguage(text=row['tweet'])\n",
    "    if prediction.language == lang and prediction.is_reliable:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def filter_by_language(df, lang='en'):\n",
    "    detector = gcld3.NNetLanguageIdentifier(min_num_bytes=50, max_num_bytes=2048)\n",
    "\n",
    "    mask = df.apply(is_lang, axis=1, detector=detector, lang=lang)\n",
    "    return df[mask]\n",
    "\n",
    "\n",
    "def is_feeling(row, sia, feeling, threshold):\n",
    "    sentiment_scores = sia.polarity_scores(row['tweet'])\n",
    "    if sentiment_scores[feeling] > threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def filter_by_sentiment(df, feeling, threshold=0.4):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    mask = df.apply(is_feeling, axis=1, sia=sia, feeling=feeling, threshold=threshold)\n",
    "    return df[mask]\n",
    "\n",
    "\n",
    "def is_relevant(row, min_likes, min_retweets):\n",
    "    try:\n",
    "        likes = float(row['likes'])\n",
    "        retweets = float(row['retweet_count'])\n",
    "        if likes > min_likes or retweets > min_retweets:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def filter_by_relevance(df, min_likes, min_retweets):\n",
    "    mask = df.apply(is_relevant, axis=1, min_likes=min_likes, min_retweets=min_retweets)\n",
    "    return df[mask]\n",
    "\n",
    "\n",
    "def preprocess_and_lemmatize(text):\n",
    "    # Tokenización de palabras y convertir texto a minúsculas\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Filtrar signos de puntuación\n",
    "    tokens = [token for token in tokens if token not in punctuation]\n",
    "    # Lematización de palabras\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Unir tokens lematizados en una cadena de texto\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "    return processed_text\n",
    "\n",
    "def clean_generated_tweets(tweets):\n",
    "    tweet_content = []\n",
    "    for text in tweets:\n",
    "        matches = re.findall(r\"tweet: (.+)\", text)\n",
    "        clean_matches = [match.strip() for match in matches]\n",
    "        tweet_content.extend(clean_matches)\n",
    "    return tweet_content"
   ],
   "metadata": {
    "id": "F_8qbPI-45eN",
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tweets filtering and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trump_df = pd.read_csv(f\"{GOOGLE_DRIVE_BASE_DIR}/hashtag_donaldtrump.csv\", sep=',', lineterminator='\\n',\n",
    "                       parse_dates=True, low_memory=False)\n",
    "biden_df = pd.read_csv(f\"{GOOGLE_DRIVE_BASE_DIR}/hashtag_joebiden.csv\", sep=',', lineterminator='\\n',\n",
    "                       parse_dates=True, low_memory=False)\n",
    "\n",
    "trump_tweets_count = len(trump_df)\n",
    "biden_tweets_count = len(biden_df)\n",
    "\n",
    "print(f'Total Trump Tweets: {trump_tweets_count}')\n",
    "print(f'Total Biden Tweets: {biden_tweets_count}')\n",
    "\n",
    "# Filtro de campos de interes\n",
    "trump_df = get_selected_columns(trump_df, COLUMNS)\n",
    "biden_df = get_selected_columns(biden_df, COLUMNS)\n",
    "\n",
    "# Filtro por longitud\n",
    "trump_df = trump_df[trump_df['tweet'].str.len() >= 50]\n",
    "biden_df = biden_df[biden_df['tweet'].str.len() >= 50]\n",
    "\n",
    "# Filtro por contenido\n",
    "# Links\n",
    "trump_df['tweet'] = trump_df['tweet'].replace(URL_REGEX, '', regex=True)\n",
    "biden_df['tweet'] = biden_df['tweet'].replace(URL_REGEX, '', regex=True)\n",
    "# Arrobas de respuesta o mencion\n",
    "trump_df['tweet'] = trump_df['tweet'].replace(TW_USERNAME_REGEX, '', regex=True)\n",
    "biden_df['tweet'] = biden_df['tweet'].replace(TW_USERNAME_REGEX, '', regex=True)\n",
    "# Espacios en blanco de mas\n",
    "trump_df['tweet'] = trump_df['tweet'].replace(SPACES_REGEX, ' ', regex=True)\n",
    "biden_df['tweet'] = biden_df['tweet'].replace(SPACES_REGEX, ' ', regex=True)\n",
    "# Caracteres inválidos\n",
    "trump_df['tweet'] = trump_df['tweet'].replace(\"&amp\", '')\n",
    "biden_df['tweet'] = biden_df['tweet'].replace(\"&amp\", '')\n",
    "# Simbolo de hashtag\n",
    "trump_df['tweet'] = trump_df['tweet'].replace(HASHTAG_REGEX, '', regex=True)\n",
    "biden_df['tweet'] = biden_df['tweet'].replace(HASHTAG_REGEX, '', regex=True)\n",
    "# Filtro por relevancia\n",
    "print(f'\\nTrump tweets mean')\n",
    "print(trump_df[['likes', 'retweet_count']].mean())\n",
    "print(f'\\nTrump tweets max')\n",
    "print(trump_df[['likes', 'retweet_count']].max())\n",
    "\n",
    "print(f'\\nBiden tweets mean')\n",
    "print(biden_df[['likes', 'retweet_count']].mean())\n",
    "print(f'\\nBiden tweets max')\n",
    "print(biden_df[['likes', 'retweet_count']].max())\n",
    "\n",
    "trump_df = filter_by_relevance(trump_df, min_likes=10, min_retweets=10)\n",
    "biden_df = filter_by_relevance(biden_df, min_likes=10, min_retweets=10)\n",
    "\n",
    "print(f\"Filtered trump tweets: {len(trump_df)}\")\n",
    "print(f\"Filtered biden tweets: {len(biden_df)}\")\n",
    "\n",
    "# Lenguage Ingles\n",
    "trump_df = filter_by_language(trump_df, lang=LANG)\n",
    "biden_df = filter_by_language(biden_df, lang=LANG)\n",
    "\n",
    "print(f\"Filtered trump tweets: {len(trump_df)}\")\n",
    "print(f\"Filtered biden tweets: {len(biden_df)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filtro por sentimiento\n",
    "trump_df = filter_by_sentiment(trump_df, feeling='neg', threshold=0.3)\n",
    "biden_df = filter_by_sentiment(biden_df, feeling='neg', threshold=0.3)\n",
    "\n",
    "print(f\"Filtered trump tweets: {len(trump_df)}\")\n",
    "print(f\"Filtered biden tweets: {len(biden_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(trump_df['tweet'][:10].to_numpy())\n",
    "print(biden_df['tweet'][:10].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Wordcloud generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Obtener lista de signos de puntuación\n",
    "punctuation = set(string.punctuation)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Trump Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocessed_trump_tweets = trump_df['tweet'].apply(preprocess_and_lemmatize)\n",
    "\n",
    "processed_trump_tweets = preprocessed_trump_tweets.str.cat(sep=' ')\n",
    "\n",
    "wordcloud = WordCloud(stopwords=stop_words).generate(processed_trump_tweets)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Biden Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocessed_biden_tweets = biden_df['tweet'].apply(preprocess_and_lemmatize)\n",
    "\n",
    "processed_biden_tweets = preprocessed_biden_tweets.str.cat(sep=' ')\n",
    "\n",
    "wordcloud = WordCloud(stopwords=stop_words).generate(processed_biden_tweets)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Post processing files generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trump_file_path = f\"{GOOGLE_DRIVE_BASE_DIR}/trump_tweets.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trump_tweets = trump_df[\"tweet\"].tolist()\n",
    "with open(trump_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    for tweet in trump_tweets:\n",
    "        file.write(f\"tweet: {tweet}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "biden_file_path = f\"{GOOGLE_DRIVE_BASE_DIR}/biden_tweets.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "biden_tweets = biden_df[\"tweet\"].tolist()\n",
    "with open(biden_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    for tweet in biden_tweets:\n",
    "        file.write(f\"tweet: {tweet}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_file_path = f\"{GOOGLE_DRIVE_BASE_DIR}/users.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(users_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    for _, tweet in biden_df.iterrows():\n",
    "        file.write(f\"username: {tweet['user_screen_name']}\\ndescription: {tweet['user_description']}\\n\")\n",
    "    for _, tweet in trump_df.iterrows():\n",
    "        file.write(f\"username: {tweet['user_screen_name']}\\ndescription: {tweet['user_description']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Trump tweets generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trump_file_path = f\"{GOOGLE_DRIVE_BASE_DIR}/trump_tweets.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = gpt2.start_tf_sess()\n",
    "\n",
    "gpt2.finetune(sess, dataset=trump_file_path, model_name='124M', steps=100, restore_from='fresh',\n",
    "              run_name='trump_tweets',\n",
    "              print_every=10, sample_every=100, save_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpt2.copy_checkpoint_to_gdrive(run_name='trump_tweets')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generated_trump_tweets = gpt2.generate(sess, length=100, temperature=0.7, nsamples=5, batch_size=5,\n",
    "                                       return_as_list=True, run_name='trump_tweets',\n",
    "                                       prefix=\"Generate a list of tweets.\\n\")\n",
    "\n",
    "for tweet in generated_trump_tweets:\n",
    "    print(tweet + '\\n\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Run from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpt2.copy_checkpoint_from_gdrive(run_name='trump_tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = gpt2.start_tf_sess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpt2.load_gpt2(sess, run_name='trump_tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generated_trump_tweets = gpt2.generate(sess, length=100, temperature=0.7, nsamples=5, batch_size=5,\n",
    "                                       return_as_list=True, run_name='trump_tweets',\n",
    "                                       prefix=\"Generate a list of tweets.\\n\")\n",
    "\n",
    "for tweet in generated_trump_tweets:\n",
    "    print(tweet + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Biden tweets generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "biden_file_path = f\"{GOOGLE_DRIVE_BASE_DIR}/biden_tweets.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = gpt2.start_tf_sess()\n",
    "\n",
    "gpt2.finetune(sess, dataset=biden_file_path, model_name='124M', steps=100, restore_from='fresh',\n",
    "              run_name='biden_tweets',\n",
    "              print_every=10, sample_every=100, save_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpt2.copy_checkpoint_to_gdrive(run_name='biden_tweets')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generated_biden_tweets = gpt2.generate(sess, length=100, temperature=0.7, nsamples=5, batch_size=5,\n",
    "                                       return_as_list=True, run_name='biden_tweets',\n",
    "                                       prefix=\"Generate a list of tweets.\\n\")\n",
    "\n",
    "for tweet in generated_biden_tweets:\n",
    "    print(tweet + '\\n\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Run from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpt2.copy_checkpoint_from_gdrive(run_name='biden_tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = gpt2.start_tf_sess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpt2.load_gpt2(sess, run_name='biden_tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generated_biden_tweets = gpt2.generate(sess, length=100, temperature=0.7, nsamples=5, batch_size=5,\n",
    "                                       return_as_list=True, run_name='biden_tweets',\n",
    "                                       prefix=\"Generate a list of tweets.\\n\")\n",
    "\n",
    "for tweet in generated_biden_tweets:\n",
    "    print(tweet + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Wordcloud generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Trump Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_generated_trump_tweets = clean_generated_tweets(generated_trump_tweets)\n",
    "preprocessed_generated_trump_tweets = [preprocess_and_lemmatize(tweet) for tweet in clean_generated_trump_tweets]\n",
    "\n",
    "processed_generated_trump_tweets = ' '.join(preprocessed_generated_trump_tweets)\n",
    "\n",
    "wordcloud = WordCloud(stopwords=stop_words).generate(processed_generated_trump_tweets)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Biden Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_generated_biden_tweets = clean_generated_tweets(generated_biden_tweets)\n",
    "preprocessed_generated_biden_tweets = [preprocess_and_lemmatize(tweet) for tweet in clean_generated_biden_tweets]\n",
    "\n",
    "processed_generated_biden_tweets = ' '.join(preprocessed_generated_biden_tweets)\n",
    "\n",
    "wordcloud = WordCloud(stopwords=stop_words).generate(processed_generated_biden_tweets)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Users generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_file_path = f\"{GOOGLE_DRIVE_BASE_DIR}/users.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aF0TR3b35UGH"
   },
   "outputs": [],
   "source": [
    "sess = gpt2.start_tf_sess()\n",
    "\n",
    "gpt2.finetune(sess, dataset=users_file_path, model_name='124M', steps=100, restore_from='fresh', run_name='users',\n",
    "              print_every=10, sample_every=100, save_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "gpt2.copy_checkpoint_to_gdrive(run_name='users')"
   ],
   "metadata": {
    "id": "p-oU-ngc5Xtf"
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users = gpt2.generate(sess, length=100, temperature=0.7, nsamples=5, batch_size=5,\n",
    "                      return_as_list=True, run_name='users',\n",
    "                      prefix=\"Generate a list of usernames and descriptions.\\n\")\n",
    "\n",
    "for user in users:\n",
    "    print(user + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Run from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HhBRmQ_2BJLF"
   },
   "outputs": [],
   "source": [
    "gpt2.copy_checkpoint_from_gdrive(run_name='users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9t5LtFyWC4BH"
   },
   "outputs": [],
   "source": [
    "sess = gpt2.start_tf_sess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpt2.load_gpt2(sess, run_name='users'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users = gpt2.generate(sess, length=100, temperature=0.7, nsamples=5, batch_size=5,\n",
    "                      return_as_list=True, run_name='users',\n",
    "                      prefix=\"Generate a list of usernames and descriptions.\\n\")\n",
    "\n",
    "for user in users:\n",
    "    print(user + '\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
