{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LTVh5gH91kL1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (1.25.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: pandas in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (2.0.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from pandas) (2023.3)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from pandas) (2023.3)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from pandas) (1.25.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\r\n",
      "^C\r\n",
      "\u001B[31mERROR: Operation cancelled by user\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: plotly in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (5.15.0)\r\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from plotly) (8.2.2)\r\n",
      "Requirement already satisfied: packaging in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from plotly) (23.1)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: pydantic in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (1.10.9)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from pydantic) (4.6.3)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: pyyaml in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (6.0)\r\n",
      "^C\r\n",
      "\u001B[31mERROR: Operation cancelled by user\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Collecting nltk\r\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.5/1.5 MB\u001B[0m \u001B[31m10.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting click\r\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m96.6/96.6 kB\u001B[0m \u001B[31m8.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting joblib\r\n",
      "  Downloading joblib-1.3.0-py3-none-any.whl (301 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m301.9/301.9 kB\u001B[0m \u001B[31m11.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting regex>=2021.8.3\r\n",
      "  Downloading regex-2023.6.3-cp311-cp311-macosx_11_0_arm64.whl (288 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m289.0/289.0 kB\u001B[0m \u001B[31m12.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting tqdm\r\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m77.1/77.1 kB\u001B[0m \u001B[31m8.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: tqdm, regex, joblib, click, nltk\r\n",
      "Successfully installed click-8.1.3 joblib-1.3.0 nltk-3.8.1 regex-2023.6.3 tqdm-4.65.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Collecting wordcloud\r\n",
      "  Downloading wordcloud-1.9.2.tar.gz (222 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m222.8/222.8 kB\u001B[0m \u001B[31m4.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.6.1 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from wordcloud) (1.25.0)\r\n",
      "Collecting pillow\r\n",
      "  Downloading Pillow-9.5.0-cp311-cp311-macosx_11_0_arm64.whl (3.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/3.1 MB\u001B[0m \u001B[31m14.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting matplotlib\r\n",
      "  Downloading matplotlib-3.7.1-cp311-cp311-macosx_11_0_arm64.whl (7.3 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.3/7.3 MB\u001B[0m \u001B[31m13.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting contourpy>=1.0.1\r\n",
      "  Downloading contourpy-1.1.0-cp311-cp311-macosx_11_0_arm64.whl (229 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m229.3/229.3 kB\u001B[0m \u001B[31m13.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting cycler>=0.10\r\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\r\n",
      "Collecting fonttools>=4.22.0\r\n",
      "  Downloading fonttools-4.40.0-cp311-cp311-macosx_10_9_universal2.whl (2.5 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m13.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting kiwisolver>=1.0.1\r\n",
      "  Downloading kiwisolver-1.4.4-cp311-cp311-macosx_11_0_arm64.whl (63 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m63.1/63.1 kB\u001B[0m \u001B[31m8.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: packaging>=20.0 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from matplotlib->wordcloud) (23.1)\r\n",
      "Collecting pyparsing>=2.3.1\r\n",
      "  Downloading pyparsing-3.1.0-py3-none-any.whl (102 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m102.6/102.6 kB\u001B[0m \u001B[31m9.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: python-dateutil>=2.7 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from matplotlib->wordcloud) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\r\n",
      "Building wheels for collected packages: wordcloud\r\n",
      "  Building wheel for wordcloud (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for wordcloud: filename=wordcloud-1.9.2-cp311-cp311-macosx_12_0_arm64.whl size=153430 sha256=7ac5f4fa49ddfed9cd613f3932472cdd42741f2dc8a49417b824c1e5fc0d15cb\r\n",
      "  Stored in directory: /Users/frbernad/Library/Caches/pip/wheels/3f/c6/5a/89824e1846baaa6d6d54b3a7b1e7deecc9ae6e7ed30a1c8b0e\r\n",
      "Successfully built wordcloud\r\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib, wordcloud\r\n",
      "Successfully installed contourpy-1.1.0 cycler-0.11.0 fonttools-4.40.0 kiwisolver-1.4.4 matplotlib-3.7.1 pillow-9.5.0 pyparsing-3.1.0 wordcloud-1.9.2\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: matplotlib in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (3.7.1)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from matplotlib) (1.1.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from matplotlib) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from matplotlib) (4.40.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from matplotlib) (1.4.4)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from matplotlib) (1.25.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from matplotlib) (23.1)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from matplotlib) (9.5.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from matplotlib) (3.1.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from matplotlib) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Collecting torch\r\n",
      "  Downloading torch-2.0.1-cp311-none-macosx_11_0_arm64.whl (55.8 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m55.8/55.8 MB\u001B[0m \u001B[31m12.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting filelock\r\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from torch) (4.6.3)\r\n",
      "Collecting sympy\r\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.7/5.7 MB\u001B[0m \u001B[31m12.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting networkx\r\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m12.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: jinja2 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\r\n",
      "Collecting mpmath>=0.19\r\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m536.2/536.2 kB\u001B[0m \u001B[31m11.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: mpmath, sympy, networkx, filelock, torch\r\n",
      "Successfully installed filelock-3.12.2 mpmath-1.3.0 networkx-3.1 sympy-1.12 torch-2.0.1\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Collecting transformers\r\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.2/7.2 MB\u001B[0m \u001B[31m13.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: filelock in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from transformers) (3.12.2)\r\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\r\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m236.8/236.8 kB\u001B[0m \u001B[31m11.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.17 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from transformers) (1.25.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from transformers) (23.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from transformers) (2023.6.3)\r\n",
      "Collecting requests\r\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.6/62.6 kB\u001B[0m \u001B[31m7.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\r\n",
      "  Downloading tokenizers-0.13.3-cp311-cp311-macosx_12_0_arm64.whl (3.9 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.9/3.9 MB\u001B[0m \u001B[31m13.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting safetensors>=0.3.1\r\n",
      "  Downloading safetensors-0.3.1-cp311-cp311-macosx_12_0_arm64.whl (401 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m401.4/401.4 kB\u001B[0m \u001B[31m13.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: tqdm>=4.27 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from transformers) (4.65.0)\r\n",
      "Collecting fsspec\r\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m163.8/163.8 kB\u001B[0m \u001B[31m9.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\r\n",
      "Collecting charset-normalizer<4,>=2\r\n",
      "  Downloading charset_normalizer-3.1.0-cp311-cp311-macosx_11_0_arm64.whl (121 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m121.7/121.7 kB\u001B[0m \u001B[31m11.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: idna<4,>=2.5 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from requests->transformers) (3.4)\r\n",
      "Collecting urllib3<3,>=1.21.1\r\n",
      "  Downloading urllib3-2.0.3-py3-none-any.whl (123 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m123.6/123.6 kB\u001B[0m \u001B[31m11.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting certifi>=2017.4.17\r\n",
      "  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m157.0/157.0 kB\u001B[0m \u001B[31m12.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: tokenizers, safetensors, urllib3, fsspec, charset-normalizer, certifi, requests, huggingface-hub, transformers\r\n",
      "Successfully installed certifi-2023.5.7 charset-normalizer-3.1.0 fsspec-2023.6.0 huggingface-hub-0.15.1 requests-2.31.0 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2 urllib3-2.0.3\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Collecting scikit-learn\r\n",
      "  Downloading scikit_learn-1.2.2-cp311-cp311-macosx_12_0_arm64.whl (8.4 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m8.4/8.4 MB\u001B[0m \u001B[31m13.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.17.3 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from scikit-learn) (1.25.0)\r\n",
      "Collecting scipy>=1.3.2\r\n",
      "  Downloading scipy-1.11.0-cp311-cp311-macosx_12_0_arm64.whl (29.5 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m29.5/29.5 MB\u001B[0m \u001B[31m13.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: joblib>=1.1.1 in /Users/frbernad/.local/share/virtualenvs/itba-nlp-kqtWtpaJ/lib/python3.11/site-packages (from scikit-learn) (1.3.0)\r\n",
      "Collecting threadpoolctl>=2.0.0\r\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\r\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\r\n",
      "Successfully installed scikit-learn-1.2.2 scipy-1.11.0 threadpoolctl-3.1.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3r57ZIm2aNl"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "71xVdhWe40br"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2LMHeadModel, get_scheduler, GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4xHHZiW92HD"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "GOOGLE_DRIVE_BASE_DIR = \"/content/drive/MyDrive\"\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Trump tweets generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trump_file_path = f\"{GOOGLE_DRIVE_BASE_DIR}/trump_tweets.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# biden = open('dataset/biden_tweets.txt').read().splitlines()\n",
    "trump = open(trump_file_path, encoding=\"utf-8\").read().splitlines()\n",
    "\n",
    "seed = 40\n",
    "\n",
    "train_data, test_data = train_test_split(trump, test_size=0.3, random_state=seed)\n",
    "\n",
    "test_data, val_data = train_test_split(test_data, test_size=0.5, random_state=seed)\n",
    "\n",
    "# Initialize the GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "  \"eos_token\": \"</s>\",\n",
    "  \"bos_token\": \"<s>\",\n",
    "  \"unk_token\": \"<unk>\",\n",
    "  \"pad_token\":  tokenizer.eos_token,\n",
    "  \"mask_token\": \"<mask>\"\n",
    "})\n",
    "\n",
    "train_tokenized = tokenizer(train_data, padding=\"max_length\", truncation=True, max_length=128)\n",
    "val_tokenized = tokenizer(val_data, padding=\"max_length\", truncation=True, max_length=128)\n",
    "test_tokenized = tokenizer(test_data, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "\n",
    "train_dataset = TweetDataset(train_tokenized)\n",
    "val_dataset = TweetDataset(val_tokenized)\n",
    "test_dataset = TweetDataset(test_tokenized)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "max_epochs = 5\n",
    "scheduler = get_scheduler(\"linear\", optimizer, num_warmup_steps=0, num_training_steps=max_epochs * len(train_loader))\n",
    "\n",
    "# Filtered trump tweets: 879\n",
    "# Filtered biden tweets: 439\n",
    "\n",
    "device = f\"cuda:{torch.cuda.current_device()}\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"input_ids\"].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}: Average training loss = {avg_train_loss}\")\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = input_ids.clone().detach()\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "    avg_eval_loss = eval_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch + 1}: Average validation loss = {avg_eval_loss}\")\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = input_ids.clone().detach()\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        test_loss += loss.item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f\"Test loss: {avg_test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = \"Trump is \"\n",
    "max_length = 140\n",
    "model.eval()\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=4,\n",
    "    temperature=0.9,\n",
    "    max_length=max_length,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    ")\n",
    "\n",
    "generated_tweets = [tokenizer.decode(tweet, skip_special_tokens=True) for tweet in output]\n",
    "\n",
    "# Print generated tweets\n",
    "for tweet in generated_tweets:\n",
    "    print(tweet)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gpt2.copy_checkpoint_to_gdrive(run_name='trump_tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generated_trump_tweets = gpt2.generate(sess, length=256, temperature=0.7, nsamples=5, batch_size=5,\n",
    "#                                        return_as_list=True, run_name='trump_tweets',\n",
    "#                                        prefix=\"Generate a list of tweets.\\n\")\n",
    "#\n",
    "# for tweet in generated_trump_tweets:\n",
    "#     print(tweet + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run from checkpoint"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# gpt2.copy_checkpoint_from_gdrive(run_name='trump_tweets')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sess = gpt2.start_tf_sess()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# gpt2.load_gpt2(sess, run_name='trump_tweets')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# generated_trump_tweets = gpt2.generate(sess, length=256, temperature=0.7, nsamples=5, batch_size=5,\n",
    "#                                        return_as_list=True, run_name='trump_tweets',\n",
    "#                                        prefix=\"Generate a list of tweets.\\n\")\n",
    "#\n",
    "# for tweet in generated_trump_tweets:\n",
    "#     print(tweet + '\\n\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Biden tweets generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# biden_file_path = f\"{GOOGLE_DRIVE_BASE_DIR}/biden_tweets.txt\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sess = gpt2.start_tf_sess()\n",
    "#\n",
    "# gpt2.finetune(sess, dataset=biden_file_path, model_name='124M', steps=1000, restore_from='fresh',\n",
    "#               run_name='biden_tweets',\n",
    "#               print_every=10, sample_every=200, save_every=500)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# gpt2.copy_checkpoint_to_gdrive(run_name='biden_tweets')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# generated_biden_tweets = gpt2.generate(sess, length=256, temperature=0.7, nsamples=5, batch_size=5,\n",
    "#                                        return_as_list=True, run_name='biden_tweets',\n",
    "#                                        prefix=\"Generate a list of tweets.\\n\")\n",
    "#\n",
    "# for tweet in generated_biden_tweets:\n",
    "#     print(tweet + '\\n\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run from checkpoint"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# gpt2.copy_checkpoint_from_gdrive(run_name='biden_tweets')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sess = gpt2.start_tf_sess()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# gpt2.load_gpt2(sess, run_name='biden_tweets')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# generated_biden_tweets = gpt2.generate(sess, length=256, temperature=0.7, nsamples=5, batch_size=5,\n",
    "#                                        return_as_list=True, run_name='biden_tweets',\n",
    "#                                        prefix=\"Generate a list of tweets.\\n\")\n",
    "#\n",
    "# for tweet in generated_biden_tweets:\n",
    "#     print(tweet + '\\n\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Users generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# users_file_path = f\"{GOOGLE_DRIVE_BASE_DIR}/users.txt\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sess = gpt2.start_tf_sess()\n",
    "#\n",
    "# gpt2.finetune(sess, dataset=users_file_path, model_name='124M', steps=1000, restore_from='fresh', run_name='users',\n",
    "#               print_every=10, sample_every=200, save_every=500)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# gpt2.copy_checkpoint_to_gdrive(run_name='users')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# users = gpt2.generate(sess, length=256, temperature=0.7, nsamples=5, batch_size=5,\n",
    "#                       return_as_list=True, run_name='users',\n",
    "#                       prefix=\"Generate a list of usernames and descriptions.\\n\")\n",
    "#\n",
    "# for user in users:\n",
    "#     print(user + '\\n\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run from checkpoint"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# gpt2.copy_checkpoint_from_gdrive(run_name='users')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# sess = gpt2.start_tf_sess()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# gpt2.load_gpt2(sess, run_name='users'),"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# users = gpt2.generate(sess, length=256, temperature=0.7, nsamples=5, batch_size=5,\n",
    "#                       return_as_list=True, run_name='users',\n",
    "#                       prefix=\"Generate a list of usernames and descriptions.\\n\")\n",
    "#\n",
    "# for user in users:\n",
    "#     print(user + '\\n\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
