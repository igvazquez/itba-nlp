{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dependencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install plotly\n",
    "!pip install pydantic\n",
    "!pip install pyyaml\n",
    "!pip install nltk\n",
    "!pip install gpt-2-simple"
   ],
   "metadata": {
    "id": "LTVh5gH91kL1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!sudo apt install -y --no-install-recommends g++ protobuf-compiler libprotobuf-dev\n",
    "!pip install gcld3"
   ],
   "metadata": {
    "id": "fJAppzUD2A8t"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "id": "J3r57ZIm2aNl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gcld3\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gpt_2_simple as gpt2\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ],
   "metadata": {
    "id": "71xVdhWe40br"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "nltk.download('vader_lexicon')\n"
   ],
   "metadata": {
    "id": "gTDrQxo0__U_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "gpt2.download_gpt2(model_name=\"124M\")"
   ],
   "metadata": {
    "id": "ScGg1cDbABP4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "COLUMNS = ['tweet', 'likes', 'retweet_count', 'user_screen_name', 'user_description', 'user_followers_count']\n",
    "LANG = 'en'\n",
    "TW_USERNAME_REGEX = r\"@[a-zA-Z0-9_]{0,15}\"\n",
    "URL_REGEX = r\"\\b(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][\" \\\n",
    "            r\"a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,\" \\\n",
    "            r\"}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\\b\"\n",
    "SPACES_REGEX = r\"\\s+\""
   ],
   "metadata": {
    "id": "S9SVbuaf420Q"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "GOOGLE_DRIVE_BASE_DIR = \"/content/drive/MyDrive/ITBA/Quinto AnÌƒo/Segundo Cuatrimestre/NLP/TP\"\n",
    "gpt2.mount_gdrive()"
   ],
   "metadata": {
    "id": "z4xHHZiW92HD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helper functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_selected_columns(df, columns):\n",
    "    return df[columns]\n",
    "\n",
    "\n",
    "def delete_hashtag_symbol(df):\n",
    "    df['tweet'] = df['tweet'].replace('#', '', regex=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def delete_twitter_username(df):\n",
    "    df['tweet'] = df['tweet'].replace(TW_USERNAME_REGEX, '', regex=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def delete_urls(df):\n",
    "    df['tweet'] = df['tweet'].replace(URL_REGEX, '', regex=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def delete_multiple_spaces(df):\n",
    "    df['tweet'] = df['tweet'].replace(SPACES_REGEX, '', regex=True)\n",
    "\n",
    "\n",
    "def is_lang(row, detector, lang='en'):\n",
    "    prediction = detector.FindLanguage(text=row['tweet'])\n",
    "    if prediction.language == lang and prediction.is_reliable:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def filter_by_language(df, lang='en'):\n",
    "    detector = gcld3.NNetLanguageIdentifier(min_num_bytes=50, max_num_bytes=2048)\n",
    "\n",
    "    mask = df.apply(is_lang, axis=1, detector=detector, lang=lang)\n",
    "    return df[mask]\n",
    "\n",
    "\n",
    "def is_negative(row, sia, threshold):\n",
    "    sentiment_scores = sia.polarity_scores(row['tweet'])\n",
    "    if sentiment_scores['neg'] > threshold:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def filter_by_sentiment(df):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    mask = df.apply(is_negative, axis=1, sia=sia, threshold=0.4)\n",
    "    return df[mask]"
   ],
   "metadata": {
    "id": "F_8qbPI-45eN"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tweets filtering and processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "trump_df = pd.read_csv(f\"{GOOGLE_DRIVE_BASE_DIR}/hashtag_donaldtrump_short.csv\", sep=',')\n",
    "biden_df = pd.read_csv(f\"{GOOGLE_DRIVE_BASE_DIR}/hashtag_joebiden_short.csv\", sep=',')\n",
    "\n",
    "# Filtro de campos de interes\n",
    "trump_df = get_selected_columns(trump_df, COLUMNS)\n",
    "biden_df = get_selected_columns(biden_df, COLUMNS)\n",
    "\n",
    "# Filtro por longitud\n",
    "trump_df = trump_df[trump_df['tweet'].str.len() >= 50]\n",
    "biden_df = biden_df[biden_df['tweet'].str.len() >= 50]\n",
    "\n",
    "# Filtro por contenido\n",
    "# Links\n",
    "trump_df['tweet'] = trump_df['tweet'].replace(URL_REGEX, '', regex=True)\n",
    "biden_df['tweet'] = biden_df['tweet'].replace(URL_REGEX, '', regex=True)\n",
    "# Arrobas de respuesta o mencion\n",
    "trump_df['tweet'] = trump_df['tweet'].replace(TW_USERNAME_REGEX, '', regex=True)\n",
    "biden_df['tweet'] = biden_df['tweet'].replace(TW_USERNAME_REGEX, '', regex=True)\n",
    "# Espacios en blanco de mas\n",
    "trump_df['tweet'] = trump_df['tweet'].replace(SPACES_REGEX, ' ', regex=True)\n",
    "biden_df['tweet'] = biden_df['tweet'].replace(SPACES_REGEX, ' ', regex=True)\n",
    "# Simbolo de hashtag\n",
    "trump_df['tweet'] = trump_df['tweet'].replace('#', '', regex=True)\n",
    "biden_df['tweet'] = biden_df['tweet'].replace('#', '', regex=True)\n",
    "# Lenguage Ingles\n",
    "trump_df = filter_by_language(trump_df, lang=LANG)\n",
    "biden_df = filter_by_language(biden_df, lang=LANG)\n",
    "\n",
    "# Filtro por sentimiento\n",
    "# trump_df = filter_by_sentiment(trump_df)\n",
    "# biden_df = filter_by_sentiment(biden_df)\n",
    "\n",
    "print(f\"Filtered trump tweets: {len(trump_df)}\\n\")\n",
    "print(f\"Filtered biden tweets: {len(biden_df)}\\n\")"
   ],
   "metadata": {
    "id": "B08vK2KI5F9k"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Post processing files generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trump_file_path = f\"{GOOGLE_DRIVE_BASE_DIR}/trump_tweets.txt\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trump_tweets = trump_df[\"tweet\"].tolist()\n",
    "with open(trump_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    for tweet in trump_tweets:\n",
    "        file.write(f\"tweet: {tweet}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "biden_file_path = f\"{GOOGLE_DRIVE_BASE_DIR}/biden_tweets.txt\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "biden_tweets = biden_df[\"tweet\"].tolist()\n",
    "with open(biden_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    for tweet in biden_tweets:\n",
    "        file.write(f\"tweet: {tweet}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "users_file_path = f\"{GOOGLE_DRIVE_BASE_DIR}/users.txt\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(users_file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    for _, tweet in biden_df.iterrows():\n",
    "        file.write(f\"username: {tweet['user_screen_name']}\\ndescription: {tweet['user_description']}\\n\")\n",
    "    for _, tweet in trump_df.iterrows():\n",
    "        file.write(f\"username: {tweet['user_screen_name']}\\ndescription: {tweet['user_description']}\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trump tweets generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sess = gpt2.start_tf_sess()\n",
    "\n",
    "gpt2.finetune(sess, dataset=trump_file_path, model_name='124M', steps=100, restore_from='fresh',\n",
    "              run_name='trump_tweets',\n",
    "              print_every=10, sample_every=100, save_every=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generated_tweets = gpt2.generate(sess, length=100, temperature=0.7, nsamples=5, batch_size=5,\n",
    "                                 return_as_list=True, run_name='trump_tweets',\n",
    "                                 prefix=\"Generate tweets with at least 20 words:\")\n",
    "\n",
    "for tweet in generated_tweets:\n",
    "    print(tweet + '\\n\\n<SEPARATOR/>\\n\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpt2.copy_checkpoint_to_gdrive(run_name='trump_tweets')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run from checkpoint"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpt2.copy_checkpoint_from_gdrive(run_name='trump_tweets')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sess = gpt2.start_tf_sess()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpt2.load_gpt2(sess, run_name='trump_tweets')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generated_tweets = gpt2.generate(sess, length=100, temperature=0.7, nsamples=5, batch_size=5,\n",
    "                                 return_as_list=True, run_name='trump_tweets',\n",
    "                                 prefix=\"Generate tweets with at least 20 words:\")\n",
    "\n",
    "for tweet in generated_tweets:\n",
    "    print(tweet + '\\n\\n<SEPARATOR/>\\n\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Biden tweets generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sess = gpt2.start_tf_sess()\n",
    "\n",
    "gpt2.finetune(sess, dataset=biden_file_path, model_name='124M', steps=100, restore_from='fresh',\n",
    "              run_name='biden_tweets',\n",
    "              print_every=10, sample_every=100, save_every=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generated_tweets = gpt2.generate(sess, length=100, temperature=0.7, nsamples=5, batch_size=5,\n",
    "                                 return_as_list=True, run_name='biden_tweets',\n",
    "                                 prefix=\"Generate tweets with at least 20 words:\")\n",
    "\n",
    "for tweet in generated_tweets:\n",
    "    print(tweet + '\\n\\n<SEPARATOR/>\\n\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpt2.copy_checkpoint_to_gdrive(run_name='biden_tweets')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run from checkpoint"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpt2.copy_checkpoint_from_gdrive(run_name='biden_tweets')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sess = gpt2.start_tf_sess()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpt2.load_gpt2(sess, run_name='biden_tweets')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generated_tweets = gpt2.generate(sess, length=100, temperature=0.7, nsamples=5, batch_size=5,\n",
    "                                 return_as_list=True, run_name='biden_tweets',\n",
    "                                 prefix=\"Generate tweets with at least 20 words:\")\n",
    "\n",
    "for tweet in generated_tweets:\n",
    "    print(tweet + '\\n\\n<SEPARATOR/>\\n\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Users generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sess = gpt2.start_tf_sess()\n",
    "\n",
    "gpt2.finetune(sess, dataset=users_file_path, model_name='124M', steps=100, restore_from='fresh', run_name='users',\n",
    "              print_every=10, sample_every=100, save_every=100)"
   ],
   "metadata": {
    "id": "aF0TR3b35UGH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "users = gpt2.generate(sess, length=100, temperature=0.7, nsamples=5, batch_size=5,\n",
    "                                  return_as_list=True, run_name='users', prefix=\"Generate a list of usernames and descriptions:\")\n",
    "\n",
    "for user in users:\n",
    "    print(user+'\\n\\n<SEPARATOR/>\\n\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "gpt2.copy_checkpoint_to_gdrive(run_name='users')"
   ],
   "metadata": {
    "id": "p-oU-ngc5Xtf"
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run from checkpoint"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "gpt2.copy_checkpoint_from_gdrive(run_name='users')"
   ],
   "metadata": {
    "id": "HhBRmQ_2BJLF"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sess = gpt2.start_tf_sess()"
   ],
   "metadata": {
    "id": "9t5LtFyWC4BH"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpt2.load_gpt2(sess, run_name='users'),"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "users = gpt2.generate(sess, length=100, temperature=0.7, nsamples=5, batch_size=5,\n",
    "                                  return_as_list=True, run_name='users', prefix=\"Generate a list of usernames and descriptions:\")\n",
    "\n",
    "for user in users:\n",
    "    print(user+'\\n\\n<SEPARATOR/>\\n\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
